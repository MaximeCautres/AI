\documentclass[8pt]{beamer}

\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{xfrac}
\usepackage{dsfont}
\usepackage{fancybox}
\usepackage{multicol}
\usepackage{color}


\usepackage{lmodern}
\usepackage{tikz}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usepackage{wrapfig}

\newcommand{\tabitem}{%
  \usebeamertemplate{itemize item}\hspace*{\labelsep}}

\def\fin#1{\leavevmode\unskip\nobreak\quad\hspace*{\fill}{#1}}

\usetheme{Warsaw}

\title[IA et conduite autonome]{Utilisation de l'intelligence artificielle dans la manœuvre autonome de bateau}
\author{Maxime CAUTRÈS}
\institute{Lycée Blaise Pascal}
\date{01/03/2020}


\AtBeginSection[]
{
  \begin{frame}
  \frametitle{Sommaire}
  \tableofcontents[currentsection, hideothersubsections]
  \end{frame} 
}

%\logo{\includegraphics[height=10mm]{logo_limos_coul_def.png}}
\logo{\includegraphics[height=15mm]{logo_noir.png}}
%\logo{\includegraphics[height=10mm]{Logo-UCA.png}}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\section{Introduction}

\subsection{Mise en contexte}


\begin{frame}{allowframebreaks}{\label{deb}}
  
  \frametitle{Données économiques}
  \begin{figure}
    \begin{center}
      \includegraphics[height=50mm]{courbe_evo_bateau.png}
      \caption{La croissance du commerce maritime international (en millions de tonnes chargées) \footnote{http://geoconfluences.ens-lyon.fr/informations-scientifiques/dossiers-regionaux/territoires-europeens-regions-etats-union/rte-t/port-anvers}}
    \end{center}
  \end{figure}
  
\end{frame}


\begin{frame}
  \frametitle{Le métier de pilote maritime}

  \begin{figure}
    \begin{minipage}[c]{.52\linewidth}
        \centering
        \includegraphics[height=5cm]{pilotemaritime.jpg}
        \caption{Transfert du pilote maritime sur le bateau à piloter \footnotemark}
    \end{minipage}
    \hfill%
    \begin{minipage}[c]{.40\linewidth}
        \centering
        \begin{itemize}
        \item Un métier \textbf{dangereux} (Le transfère du pilote)\break
          \pause
        \item Un \textbf{coût matériel} important (Bateau ou hélicoptère)\break
          \pause
        \item Un \textbf{coût financier} important (7\% du coût de l'escale)
        \end{itemize}
    \end{minipage}
    \footnotetext{http://escale.sinerj.org/spip.php?article41}
\end{figure}
  
\end{frame}


\begin{frame}
  \frametitle{Étude de l'existant}

   \begin{figure}
    \begin{minipage}[c]{.46\linewidth}
        \centering
        \includegraphics[width=55mm]{boat_docking.jpg}
        \caption{Vu aérienne de la trajectoire suivit par l'asservissement du bateau \footnotemark}
    \end{minipage}
    \footnotetext{https://smartmaritimenetwork.com/2019/02/08/yanmar-trials-robotic-ship-technology/}
    \hfill%
    \begin{minipage}[c]{.46\linewidth}
        \centering
        \begin{itemize}
        \item \textbf{Peu d'acteurs} dans le domaine (Deux principaux avec Yanmar et Volvo) \break
          \pause
        \item Nécessite des modifications importantes des \textbf{infrastructures} (capteurs, antennes) \break
          \pause
        \item Un dispositif \textbf{très lent et peu adapté} aux déplacements important dans un port 
        \end{itemize}
    \end{minipage}
     
\end{figure}
\end{frame}

\subsection{Une nouvelle approche}

\begin{frame}

  \frametitle{L'apprentissage automatique:}

  \begin{figure}
    \begin{minipage}[c]{.46\linewidth}
      \centering
      \includegraphics[width=55mm]{simulation_boat.jpg}
    \end{minipage}
    \hfill%
    \begin{minipage}[c]{.46\linewidth}
      \centering
      \begin{itemize}
  \item Un environnement pour simuler les conditions réelles \break
    \break \break \break
  \end{itemize}
    \end{minipage}
  \end{figure}

\end{frame}
  
\begin{frame}

  \frametitle{L'apprentissage automatique:}

  \begin{figure}
    \begin{minipage}[c]{.46\linewidth}
      \centering
      \includegraphics[width=45mm]{network_example.png}
    \end{minipage}
    \hfill%
    \begin{minipage}[c]{.46\linewidth}
      \centering
      \begin{itemize}
  \item Un environnement  \break
  \item La techonologie des réseaux de neurones\break
    \pause
  \item Des algorithmes d'entrainement
  \end{itemize}
    \end{minipage}
  \end{figure}

\end{frame}

\subsection{Problématique}

\begin{frame}

  \frametitle{Problématique}
  
  Comment peut-on utiliser l’\textbf{apprentissage automatique} pour
  permettre à un bateau de \textbf{manœuvrer dans un port} dans le but de
  minimiser les dépenses liées à l’augmentation du trafic tout en
  garantissant la sécurité ?
  \break \pause
  \begin{center}
    \begin{tabular}{@{}l@{}}
      Le plan: \\ \\ 
      \pause
      \tabitem Première approche avec le Q-learning \\ \\
      \pause
      \tabitem Simulation de l'environnement portuaire \\ \\
      \pause
      \tabitem Seconde approche avec le Policy Gradients
    \end{tabular}
    \end{center}
\end{frame}



\section{Le Q-learning}
\subsection{Le problème des souris}

\begin{frame}{allowframebreaks}{\label{2}}
  \frametitle{Un problème intermédiaire pour se lancer}
  \framesubtitle{Description}

  \begin{figure}
    \begin{minipage}[c]{.46\linewidth}
      \centering
      \includegraphics[width=45mm]{map_q-learning.png}
    \end{minipage}
    \hfill%
    \begin{minipage}[c]{.46\linewidth}
      \centering
      \begin{itemize}
      \item En Noir les obstacles \break
      \item En blanc les cases accessibles \break
      \item La souris est en bleu \break
      \item L'objectif en la case en bas a droite \break
  \end{itemize}
    \end{minipage}
  \end{figure}
 
\end{frame}

\begin{frame}{allowframebreaks}{\label{2}}
  \frametitle{Un problème intermédiaire pour se lancer}
  \framesubtitle{Formalisation}
  \begin{figure}
    \begin{minipage}[c]{.55\linewidth}
      \centering
      \input{markov.tex}
    \end{minipage}
    \hfill%
    \begin{minipage}[c]{.37\linewidth}
      \centering
      \begin{itemize}
      \item On utilise \textbf{les chaines de Markov deterministe}
    \item {\color{blue} Bleu} pour l'état initial
    \item {\color{green} Vert} pour l'état final
    \item {\color{red} Rouge} pour les murs
    \item \textbf{a, b, c, d} pour les actions
    \item Un système de \textbf{récompense}
      \end{itemize}
    \end{minipage}
  \end{figure}
\end{frame}



\begin{frame}{allowframebreaks}{\label{2}}
  \frametitle{Un problème intermédiaire pour se lancer}
  \framesubtitle{Définitions}
  \begin{figure}
    
    \begin{minipage}[c]{.55\linewidth}
      \centering
      \input{markov.tex}
    \end{minipage}
    \hfill%
    \begin{minipage}[c]{.37\linewidth}
      \centering
      Le Q-learing:
      \begin{itemize}
      \item Une fonction de valuation: \begin{equation} V^{\pi}(s, a) \end{equation}
        \item Pour se déplacer:
          \begin{equation}{\label{mouvementmdp}}
            s' = \max_{a}(V^{\pi}(s, a))
          \end{equation}
        \item La récompense: \begin{equation} R(s, a) \end{equation}
      \end{itemize}
    \end{minipage}
  \end{figure}
\end{frame}


\begin{frame}

  \frametitle{Algorithme et équation de Bellman}

  \begin{block}{Initialisation}
    On définit les $V^{\pi}(s, a)$ aléatoirement
  \end{block}

  \begin{block}{Récurrence}
    \begin{itemize}
    \item On effectue une simulation grâce à la formule (\ref{mouvementmdp})
    \item Sur chaque état alors visité, on applique l'équation de Bellman:
     \begin{align}
       V^{\pi}_{t+1}(s, a)&=R(s, a)+\gamma \sum _{s'}P(s'|s, a)V^{\pi }_{t}(s') \\
       \Leftrightarrow\footnotemark V^{\pi}_{t+1}(s, a)&=R(s, a)+\gamma V_t^{\pi}(s')
     \end{align}
     
    \end{itemize}
    \footnotetext{Ici l'équivalence vient du fait que l'environnement est déterministe}
  \end{block}
  
  \begin{block}{Terminaison}
    On arrête l'algorithme une solution optimal est trouvée ou si une limite de temps est dépassée
  \end{block}

\end{frame}


\begin{frame}
  \frametitle{Performance de la méthode}
  \begin{center}
    ici, il faut une image des performance au cours du temps sur le Q learning, je n'en ai pas trouver
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Limite de la méthode}

  \begin{block}{Physique}
    \begin{itemize}
    \item Temps d'exécution
    \item Faible adaptivité
    \item Difficulté malgrès l'environnement simple
    \end{itemize}
  \end{block}

  \pause

  \begin{block}{Amélioration}
    \begin{itemize}
    \item Un environnement plus réaliste
    \item Une meilleur adaptivité
    \item Une vitesse de calcul plus importante
    \end{itemize}
  \end{block}
  
  
\end{frame}

\section{L'environnement}

\subsection{Le cahier des charges}

\begin{frame}

  \frametitle{Objectifs et contraintes}

\begin{block}{Objectif}
    \begin{itemize}
    \item Prise en compte de l'inertie
    \item Prise en compte des frottements visqueux
    \item Prise en compte des caractéristiques physiques du bateau
    \item Un environnement qui représente un port
    \end{itemize}
\end{block}

\pause

\begin{block}{Contrainte}
    \begin{itemize}
    \item Le modèle doit être très rapide d'exécution
    \item Autoriser l'exécution en parallèle
    \item Être représentable visuellement
    \end{itemize}
\end{block}

\end{frame}

\subsection{Notre implémentation}

\begin{frame}

  \frametitle{Visuellement}

  \begin{figure}
    \begin{minipage}[c]{.55\linewidth}
      \centering
      \includegraphics[width=55mm]{map_v.png}
      \caption{Rendu visuel de notre environnement \footnotemark}
    \end{minipage}
    \hfill%
    \begin{minipage}[c]{.37\linewidth}
      \centering
      \begin{itemize}
      \item Un environnement \textbf{discrétisé}
    \item Blanc pour le bateau
    \item Beige pour les murs
    \item Violet pour l'objectif
    \item Les cases rouges et vertes \textbf{montre les actions}
      \end{itemize}
    \end{minipage}
  \end{figure}
  \footnotetext{Les actions sont \textbf{seulement affichées pour l'utilisateur}, elles ne font pas parties de l'environnement}

\end{frame}

\begin{frame}

  \frametitle{Visuellement}

  \begin{figure}
    \begin{minipage}[c]{.46\linewidth}
      \centering
      \input{trajec_example.tex}
      \caption{Exemple de trajectoire de bateau}
    \end{minipage}
    \hfill%
    \begin{minipage}[c]{.46\linewidth}
      \centering
      \begin{itemize}
      \item En {\color{red} rouge} les positions successives du bateau
      \item En {\color{purple} violet}, prise en compte de l'inertie (répétition du déplacement)
      \item En {\color{green} vert} les choix d'actions successifs (rayon 1 de autour de {\color{purple}violet} \footnotemark)
      \item Sous python, \textbf{Numpy} permet la \textbf{vectorisation} et donc les \textbf{parties simultanées} (1000 parties prennent le même temps que une ou deux parties)
      \end{itemize}
    \end{minipage}
  \end{figure}
  \footnotetext{Ici, la zone est carré mais la forme peut varier pour augmenter l'aspect réaliste du modèle et s'adapter aux caractéristiques même du bateau.}

\end{frame}


\section{Le Policy Gradients}

\subsection{La théorie}

\begin{frame}

  \frametitle{Définissions et notations}

  \begin{block}{La Politique et ses fonctions Gain, Q-Value, Value et Reward associées}
    \begin{itemize}
    \item La Politique
      \begin{equation} \pi_{\theta}(s) = (p_i)_{i \in \llbracket 1, ac \rrbracket} / {\scalebox{0.85}{$\displaystyle \sum_{i = 1}^{ac}$}}p_i = 1  \end{equation}
    \item Le Gain
      \begin{equation} G_t = {\scalebox{0.85}{$\displaystyle \sum_{k=0}^{\infty}$}} \gamma^k R_{t+k+1} \end{equation}
    \item La Q-value
      \begin{equation} Q^\pi(s, a) = \mathbb{E}_{a\sim \pi} [G_t \vert S_t = s, A_t = a] \end{equation}
    \item La Value
      \begin{equation} V^\pi (s) = \mathbb{E}_{a\sim \pi} [G_t \vert S_t = s] \end{equation}
    \item La Recompense, fonction Reward
      \begin{equation} J(\theta) 
= {\scalebox{0.85}{$\displaystyle \sum_{s}$}} d^\pi(s) V^\pi(s) 
= {\scalebox{0.85}{$\displaystyle \sum_{s}$}} d^\pi(s) {\scalebox{0.85}{$\displaystyle  \sum_{a}$}} \pi_\theta(a \vert s) Q^\pi(s, a)
= \mathbb{E}_{\pi} [Q^{\pi}(s,a)] \end{equation}
    \end{itemize}
  \end{block}
  
\end{frame}

\begin{frame}[label=retourCNN]

  \frametitle{La Politique}

  \begin{block}{Un réseau de neurones de convolution maison\footnotemark pour $\pi_{\theta}(s)$}
    \begin{itemize}
    \item $s$ correspond à l'entrée
    \item $\pi_{\theta}(s)$ correspond à la sortie
    \item $\theta$ correspond aux poids et biais du réseau
    \end{itemize}

  \end{block}

  \begin{figure}
    
    \begin{minipage}[c]{.28\linewidth}
      \centering
      \includegraphics[width=\linewidth]{map_v.png}
    \end{minipage}
    \hfill%
    \begin{minipage}[c]{.04\linewidth}
      \centering
      $=$
    \end{minipage}
    \hfill%
    \begin{minipage}[c]{.43\linewidth}
      \centering
      \includegraphics[width=1.05\linewidth]{CNN.png}
    \end{minipage}
    \hfill%
    \begin{minipage}[c]{.218\linewidth}
      \centering \[ =
      \left[
      \begin{array}{c}
        {\color{red}0.01}\\
        {\color{green} 0.85}\\
        {\color{red}0.01}\\
        {\color{red}0.03}\\
        {\color{red}0.04}\\
        {\color{red}0.01}\\
        {\color{red}0.02}\\
        {\color{red}0.01}\\
        {\color{red}0.02}
      \end{array}
      \right] \]
    \end{minipage}
  \end{figure}
  \footnotetext{Nous utilisons ici \hyperlink{CNN}{notre implémentation sans librairie spécialisée du Convolutional Neural Network}}
\end{frame}

\begin{frame}[label=policyintroduction]
  \frametitle{L'entrainement}
  \begin{block}{L'initialisation}
    On définit une structure pour le réseau de neurones où les poids et biais sont définis aléatoirement
  \end{block}

  \begin{block}{Par récurrence (En époques) \footnotemark}
    \begin{itemize}
    \item On effectue P parties en parallèle, on récupère:
      \begin{center}
      {\scalebox{0.85}{
      $\left\{
      \begin{array}{c}
        S_0^1, A_0^1, R_0^1, \cdots, S_{f_1-1}^1, A_{f_1-1}^1, R_{f_1-1}^1, S^1_{f_1} \\
        \vdots \\
        S_0^P, A_0^P, R_0^P, \cdots, S_{f_P-1}^P, A_{f_P-1}^P, R_{f_P-1}^P, S^P_{f_P} 
      \end{array}
      \right. $}} \end{center}
    \item Pour tout $i \in \llbracket 1, P \rrbracket$ et $t \in \llbracket 0, f_i - 1 \rrbracket$
      \begin{align}
        G_t^i &= {\scalebox{0.85}{$\displaystyle \sum_{k=O}^{f_i-t-2}$}} \gamma^k R^i_{t+k+1} \\
        \theta &\leftarrow \theta + \alpha \gamma^t G^i_t \nabla_\theta \ln \pi_\theta(A^i_t \vert S^i_t)
      \end{align}
    
    \end{itemize}

  \end{block}

  \footnotetext{Ceci résulte d'un \hyperlink{demopolicygradient}{théorème majeur sur le Policy Gradients}}
\end{frame}

\begin{frame}
  \frametitle{Quelques précisions sur les gradients: La rétropropagation}

  \begin{block}{Initialisation}
    On calcul les gradients de la dernière couche du réseau 
  \end{block}

  \begin{block}{Par récurrence}
      On rétro propage les gradients sur l'ensemble du réseau de neurones grâce à l'astuce:
      \[\frac{\partial \pi_{\theta}(s)}{\partial \theta} = \frac{\partial \pi_{\theta}(s)}{\partial a} \cdot \frac{\partial a}{\partial \theta} \]
      Ce qui nous permet en accumulant ce principe de remonter couches par couches le réseau de neurones.
  \end{block}
\end{frame}

\subsection{Résultats}

\begin{frame}
  \frametitle{Une implémentation naïve de Policy Gradient}
  \framesubtitle{Avec un DNN}

  \begin{figure}
    \begin{minipage}[c]{.55\linewidth}
      \centering
      \includegraphics[width=55mm]{parameters_3-5h31m46s.png}
      \caption{135000 époques de 100 parties, 82.35\% de réussite en 5h31m46s sur un cœur de CPU à 3,7 GHz}
    \end{minipage}
    \hfill%
    \begin{minipage}[c]{.40\linewidth}
      \centering
      \begin{itemize}
      \item En {\color{red}rouge} les performances moyennes  \break
      \item En {\color{blue}bleu} la variance moyenne \break
      \end{itemize}
    \end{minipage}
  \end{figure}

\end{frame}

\begin{frame}
  \frametitle{Une implémentation naïve de Policy Gradient}
  \framesubtitle{Avec un CNN}

  \begin{figure}
    \begin{minipage}[c]{.46\linewidth}
      \centering
      \includegraphics[height=30mm]{97-33_2.png}
      \caption{42000 époques de 200 parties, 97.49\% de réussite en 19h26m27s sur 1 cœur de CPU à 3.7GHz}
    \end{minipage}
    \hfill%
    \begin{minipage}[c]{.46\linewidth}
      \centering
      \includegraphics[height=30mm]{98-47_4.png}
      \caption{42000 époques de 200 parties, 98.54\% de réussite en 18h27m42s sur 1 cœur de CPU à 3.7GHz}
    \end{minipage}
  \end{figure}

\end{frame}

\begin{frame}
  \frametitle{Une implémentation plus juste du Policy Gradient}
  \framesubtitle{Avec un CNN}
  Ici mettre les futures courbes
\end{frame}


\section*{Conclusion}

\begin{frame}{Objectif}
  \begin{itemize}
    \item Réussir à faire stationner un bateau dans un port \break
\item Programmer notre propre algorithme d'apprentissage par renforcement sans utiliser de librairie dédiée. \fin{\huge \checkmark} \break
\item Créer une simulation discrète et réaliste d'un déplacement de bateau prenant en compte l'inertie et la viscosité. \fin{\huge \checkmark} \break
\item Comprendre et réussir à manipuler les concepts sur lesquels sont basés l'intelligence artificielle. \fin{\huge \checkmark} \break
\item Implémenter différentes technologies pour pouvoir comparer les performances et trouver la meilleure solution technique à notre problème. \fin{\huge $\sim$}
  \end{itemize}
\end{frame}

\begin{frame}{Ouverture}
  \begin{itemize}
  \item Implémentation de l'Actor Critic
  \item Meilleurs algorithmes
  \item Simulation encore plus réaliste
  \end{itemize}
\end{frame}


\appendix

\section*{Notre implémentation du CNN}

\begin{frame}[label=CNN]
  \frametitle{Un fait maison}
  \begin{center}
    Il faudra peut être détailler ici le CNN et DNN et qu'est ce que le fait main"
    \end{center}
  \hyperlink{retourCNN}{Retour}
\end{frame}

\section*{Preuve du Policy Gradient Theroem (dans le cas épisodique)}

\begin{frame}[label=demopolicygradient]
  \frametitle{Développement de $\nabla_{\theta} V^{\pi}(s)$}

%  \hyperlink{policyintroduction}{Retour}
  
    \begin{align*}
      \nabla_{\theta}V^{\pi}(s) &= \nabla_{\theta} \big[ {\scalebox{0.85}{$\sum_a$}} \pi_{\theta}(a|s)Q^{\pi}(s,a) \big] \\
      &= {\scalebox{0.85}{$\sum_a$}} \big[\nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a) + \pi_{\theta}(a|s) \nabla_{\theta} Q^{\pi}(s,a) \big] \\
      &= {\scalebox{0.85}{$\sum_a$}} \big[\nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a) + \pi_{\theta}(a|s) \nabla_{\theta} {\scalebox{0.85}{$\sum_{s',r}$}}p(s', r |s,a)(r+V^{\pi}(s')) \big] \\
      &= {\scalebox{0.85}{$\sum_a$}} \big[\nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a) + \pi_{\theta}(a|s) \nabla_{\theta} {\scalebox{0.85}{$\sum_{s'}$}}p(s'|s,a)\nabla_{\theta}V^{\pi}(s')) \big] \\
      &= \Phi(s) + {\scalebox{0.85}{$\sum_a$}} \big[\pi_{\theta}(a|s) \nabla_{\theta} {\scalebox{0.85}{$\sum_{s'}$}}p(s'|s,a)\nabla_{\theta}V^{\pi}(s')) \big] \\
      &= \Phi(s) + {\scalebox{0.85}{$\sum_{s'}$}}{\scalebox{0.85}{$\sum_a$}} \big[\pi_{\theta}(a|s) \nabla_{\theta} p(s'|s,a)\nabla_{\theta}V^{\pi}(s')) \big] \\
      &= \Phi(s) + {\scalebox{0.85}{$\sum_{s'}$}}\rho^{\pi}(s\rightarrow s', 1)\nabla_{\theta}V^{\pi}(s')) \\
      &= \Phi(s) + {\scalebox{0.85}{$\sum_{s'}$}}\rho^{\pi}(s\rightarrow s', 1)\nabla_{\theta} \big[ {\scalebox{0.85}{$\sum_a$}} \pi_{\theta}(a|s)Q^{\pi}(s',a) \big] \\
      &= \Phi(s) + {\scalebox{0.85}{$\sum_{s'}$}}\rho^{\pi}(s\rightarrow s', 1)\Phi(s') + {\scalebox{0.85}{$\sum_{s''}$}}\rho^{\pi}(s\rightarrow s'', 1)\nabla_{\theta}V^{\pi}(s'')) \\
      &= \cdots \\
      &= {\scalebox{0.85}{$\sum_{\tilde s}$}}{\scalebox{0.85}{$\sum_{k}$}}\rho^{\pi}(s \rightarrow \tilde s, k)\phi(\tilde s)
    \end{align*}

\end{frame}

 %begin{frame}
% \frametitle{Développement de $\nabla_\theta J(\theta)$}
%  Ici, nous devons supposer que les parties sont finies
%  \begin{align*}
%    \nabla_\theta J(\theta) &= \nabla_{\theta} {\scalebox{0.85}{$\sum_{s}$}} d^\pi(s) V^\pi(s) \\
%    &= \nabla_{\theta}V^{\pi}(s_0) \\
%    &= {\scalebox{0.85}{$\sum_{s}$}}{\scalebox{0.85}{$\sum_{k}$}}\rho^{\pi}(s_0 \rightarrow s, k)\phi( s)\\
%    &= \big[{\scalebox{0.85}{$\sum_{s}$}}{\scalebox{0.85}{$\sum_{k}$}}\rho^{\pi}(s_0 \rightarrow s, k)\big] {\scalebox{0.85}{$\sum_{s}$}}\frac{{\scalebox{0.85}{$\sum_{k}$}}\rho^{\pi}(s_0 \rightarrow s, k)}{{\scalebox{0.85}{$\sum_{s}$}}{\scalebox{0.85}{$\sum_{k}$}}\rho^{\pi}(s_0 \rightarrow s, k)}\phi(s)\\
%    &= \big[{\scalebox{0.85}{$\sum_{s}$}}{\scalebox{0.85}{$\sum_{k}$}}\rho^{\pi}(s_0 \rightarrow s, k)\big] {\scalebox{0.85}{$\sum_{s}$}}d^{\pi}(s)\phi(s)\\
%    &= \propto {\scalebox{0.85}{$\sum_{s}$}}d^{\pi}(s)\phi(s)
%  \end{align*}
%  \hyperlink{policyintroduction}{Retour}
%\end{frame}

\begin{frame}
  \frametitle{Développement de $\nabla_\theta J(\theta)$}
  Ici, nous devons supposer que les parties sont finies (i.e. $k \in \llbracket 0, f_i - 1 \rrbracket$ au lieu de $k \in \llbracket 0, +\infty \llbracket$)
  \begin{align*}
    \nabla_\theta J(\theta) &= \nabla_{\theta} {\scalebox{0.85}{$\sum_{s_0}$}} d^\pi(s_0) V^\pi(s_0) \\
    &= \nabla_{\theta}{\scalebox{0.85}{$\sum_{s_0}$}}p(s_0)V^{\pi}(s_0) \\
    &= {\scalebox{0.85}{$\sum_{s_0}$}}p(s_0)\nabla_{\theta}V^{\pi}(s_0) \\
    &= {\scalebox{0.85}{$\sum_{s_0}$}}p(s_0){\scalebox{0.85}{$\sum_{s}$}}{\scalebox{0.85}{$\sum_{k=0}^{f_i - 1}$}}\rho^{\pi}(s_0 \rightarrow s, k)\phi( s)\\
    &= {\scalebox{0.85}{$\sum_{s_0}$}}p(s_0)\big[{\scalebox{0.85}{$\sum_{k=0}^{f_i - 1}$}}{\scalebox{0.85}{$\sum_{s}$}}\rho^{\pi}(s_0 \rightarrow s, k)\big] {\scalebox{0.85}{$\sum_{s}$}}\frac{{\scalebox{0.85}{$\sum_{k}^{f_i}$}}\rho^{\pi}(s_0 \rightarrow s, k)}{{\scalebox{0.85}{$\sum_{s}$}}{\scalebox{0.85}{$\sum_{k=0}^{f_i-1}$}}\rho^{\pi}(s_0 \rightarrow s, k)}\phi(s)\\
    &= {\scalebox{0.85}{$\sum_{s_0}$}}p(s_0)\big[{\scalebox{0.85}{$\sum_{k=0}^{f_i-1}$}}1\big] {\scalebox{0.85}{$\sum_{s}$}}d^{\pi}(s)\phi(s)\\
    &= {\scalebox{0.85}{$\sum_{s_0}$}}p(s_0)f_i {\scalebox{0.85}{$\sum_{s}$}}d^{\pi}(s)\phi(s)\\
    &= {\scalebox{0.85}{$\sum_{s_0}$}}p(s_0)f_i {\scalebox{0.85}{$\sum_{s}$}}d^{\pi}(s) {\scalebox{0.85}{$\sum_{a}$}}\nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a)\\
    &= f_i\big[{\scalebox{0.85}{$\sum_{s_0}$}}p(s_0)\big] \big[ {\scalebox{0.85}{$\sum_{s}$}}d^{\pi}(s) {\scalebox{0.85}{$\sum_{a}$}} \nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a)\big]\\
    &= f_i\big[{\scalebox{0.85}{$\sum_{s}$}}d^{\pi}(s) {\scalebox{0.85}{$\sum_{a}$}} \nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a)\big]
  \end{align*}
  
\end{frame}

\begin{frame}
  \frametitle{Utilisation de $\nabla_\theta J(\theta)$}
  Si l'on suppose maintenant que tout les parties ont une durée proche:
  \begin{align*}
    \nabla_\theta J(\theta) &= f_i\big[{\scalebox{0.85}{$\sum_{s}$}}d^{\pi}(s) {\scalebox{0.85}{$\sum_{a}$}} \nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a)\big] \\
    &= \propto \big[{\scalebox{0.85}{$\sum_{s}$}}d^{\pi}(s) {\scalebox{0.85}{$\sum_{a}$}} \nabla_{\theta}\pi_{\theta}(a|s)Q^{\pi}(s,a)\big] \\
    &= \propto \big[{\scalebox{0.85}{$\sum_{s}$}}d^{\pi}(s) {\scalebox{0.85}{$\sum_{a}$}} \pi_{\theta}(a|s) \frac{\nabla_{\theta}\pi_{\theta}(a|s)}{\pi_{\theta}(a|s)}Q^{\pi}(s,a)\big] \\
    &= \propto \big[{\scalebox{0.85}{$\sum_{s}$}}d^{\pi}(s) {\scalebox{0.85}{$\sum_{a}$}} \pi_{\theta}(a|s) \nabla_{\theta}Q^{\pi}(s,a)\ln \pi_{\theta}(a|s) \big] \\
    &= \mathbb{E}_{s \sim d^{\pi},a \sim\pi_{\theta}} [Q^{\pi}(s,a)\nabla_{\theta} \ln \pi_{\theta}(a|s)]
  \end{align*}

  Il faut maintenant revenir à la machine qui nous permettra d'obtenir $A_t^i, S_t^i, R_t^i$:

  \begin{align*}
    \nabla_\theta J(\theta) &= \mathbb{E}_{s \sim d^{\pi},a \sim\pi_{\theta}} [Q^{\pi}(s,a)\nabla_{\theta} \ln \pi_{\theta}(a|s)] \\
    &= \mathbb{E}_{s \sim d^{\pi},a \sim\pi_{\theta}} [G_t^i \nabla_{\theta} \ln \pi_{\theta}(A^i_t|S^i_t)]
  \end{align*}

  On cherche à augmenter $J(\theta)$ d'où la monté de gradient, d'où des modifications sur les paramètres selon le gradients, il ne faut pas oublier de recoefficienter le tout en fonction de la temporalité avec $\gamma^t$:
  \[ \theta \leftarrow \theta + \alpha \gamma^t G^i_t \nabla_\theta \ln \pi_\theta(A^i_t \vert S^i_t) \]

  \hyperlink{policyintroduction}{Retour}
\end{frame}


\end{document}

