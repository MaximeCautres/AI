\relax 
\catcode `:\active 
\catcode `;\active 
\catcode `!\active 
\catcode `?\active 
\babel@aux{french}{}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Solutions technologiques}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Courte \IeC {\'e}tude de l'existant}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Qu'est ce que l'intelligence artificielle ?}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Un peu d'histoire}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Les grandes id\IeC {\'e}es}{4}\protected@file@percent }
\citation{s4a}
\citation{3b1b}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Le Projet}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Les notions:}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Les fonctions $F^*$ et $f$}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}La fonction erreur $E$}{6}\protected@file@percent }
\newlabel{error}{{1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Repr\IeC {\'e}sentation d'une fonction erreur quelconque (Surface) avec des chemins la minimisant (Courbes)}}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}L'apprentissage}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Les diff\IeC {\'e}rents types d'algorithmes}{6}\protected@file@percent }
\citation{mnist}
\citation{cifar}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Les bases de donn\IeC {\'e}es}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Premier algorithme:}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Deep Neural Networks:}{7}\protected@file@percent }
\newlabel{compute}{{2}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Sch\IeC {\'e}ma d'un r\IeC {\'e}seau de neurones}}{8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Sch\IeC {\'e}ma d'un perceptron (neurone) isol\IeC {\'e}}}{8}\protected@file@percent }
\newlabel{compute_vect}{{3}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evolution de l'algorithme}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}D\IeC {\'e}couverte de la technologie}{8}\protected@file@percent }
\citation{steve}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Familiarisation avec la r\IeC {\'e}tro-propagation}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Les notations}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {En rouge:} Descente de gradients conventionnelle tr\IeC {\`e}s directe mais lente. \textbf  {En noir:} Exemple de r\IeC {\'e}tro-propagation, moins directe mais bien plus rapide.}}{9}\protected@file@percent }
\newlabel{z}{{4}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Les \IeC {\'e}quations math\IeC {\'e}matiques}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Les mini-batchs}{10}\protected@file@percent }
\newlabel{up_w}{{10}{11}}
\newlabel{up_b}{{11}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}L'algorithme d'apprentissage:}{11}\protected@file@percent }
\citation{optimiseur1}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Algorithme final}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Fonctions d'activations}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Les optimiseurs}{12}\protected@file@percent }
\citation{optimiseur2}
\citation{optimiseur3}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Le Drop out et principe de sur-apprentissage}{13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \IeC {\'E}volution de l'erreur durant l'apprentissage sans Drop Out}}{14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \IeC {\'E}volution de l'erreur durant l'apprentissage avec Drop Out mal impl\IeC {\'e}ment\IeC {\'e} }}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}R\IeC {\'e}sultat}{14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces R\IeC {\'e}sultat du meilleur entrainement sans drop out sur MNIST (100\% et 94.5\%)}}{15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces R\IeC {\'e}sultat du meilleur entrainement avec drop out sur MNIST (99.1\% et 96.1\%)}}{15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Courbes de performance en fonction des taux de drop out}}{15}\protected@file@percent }
\citation{cnn}
\@writefile{toc}{\contentsline {part}{IV\hspace  {1em}Second algorithme}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Convolutional Neural Networks:}{16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Lien entre deux couches du r\IeC {\'e}seau de convolution.}}{17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Exemple de calcul d'un produit de convolution.}}{17}\protected@file@percent }
\newlabel{CNN}{{16}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Illustrations selon 3 fonctions de pooling}}{18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Sch\IeC {\'e}ma du cerveau artificiel complet}}{19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Sch\IeC {\'e}ma pour deux formats de kernel et de strides}}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}\IeC {\'E}volution}{20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Prolongement et adaptation de la r\IeC {\'e}tro-propagation}{20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}Les notations}{20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Les \IeC {\'e}quations math\IeC {\'e}matiques}{20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.3}L'algorithme d'apprentissage}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Algorithme final et Batch-Normalization}{21}\protected@file@percent }
\citation{bn}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Courbe d'illustration du sur-apprentissage}}{22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Images issuent de MNIST}}{22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Images issuent de CIFAR 10}}{22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Sch\IeC {\'e}ma du cerveau artificiel complet}}{23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}R\IeC {\'e}sultats}{23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Performance durant l'entrainement sur CIFAR15}}{25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Ensemble des cat\IeC {\'e}gories et sous cat\IeC {\'e}gories de CIFAR100}}{25}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{V\hspace  {1em}Conclusion G\IeC {\'e}n\IeC {\'e}rale}{26}\protected@file@percent }
\bibstyle{plain}
\bibdata{data_base}
\bibcite{3b1b}{1}
\bibcite{bn}{2}
\bibcite{cifar}{3}
\bibcite{mnist}{4}
\bibcite{s4a}{5}
\bibcite{steve}{6}
\bibcite{optimiseur3}{7}
\bibcite{optimiseur2}{8}
\bibcite{optimiseur1}{9}
\bibcite{cnn}{10}
\@writefile{toc}{\contentsline {part}{VI\hspace  {1em}Preuves}{28}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}D\IeC {\'e}monstration des \IeC {\'e}quations de la r\IeC {\'e}tro-propagation DNN}{28}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}D\IeC {\'e}monstration des \IeC {\'e}quations de la r\IeC {\'e}tro-propagation CNN}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}D\IeC {\'e}monstration des \IeC {\'e}quations de r\IeC {\'e}tro-propagation Batch-normalisation}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{VII\hspace  {1em}Algorithmes:}{33}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Algorithme interface et sauvegarde:}{33}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}Algorithme d'ex\IeC {\'e}cution du r\IeC {\'e}seau de neurones}{35}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15}Algorithme de r\IeC {\'e}tro-propagation}{37}\protected@file@percent }
